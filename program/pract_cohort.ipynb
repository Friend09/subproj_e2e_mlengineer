{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRACTISE CODE OF SESSIONS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session 1 - Introduction and Initial Setup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- there are 4 pipelines as part of this framework\n",
    "  - training\n",
    "  - inference\n",
    "  - deployment\n",
    "  - monitoring\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "The dotenv extension is already loaded. To reload it, use:\n",
      "  %reload_ext dotenv\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext dotenv\n",
    "%dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import ipytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('code')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CODE_FOLDER = Path(\"code\")\n",
    "sys.path.extend([f\"./{CODE_FOLDER}\"])\n",
    "\n",
    "# Folder at which the code scripts are saved\n",
    "CODE_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILEPATH = \"penguins.csv\"\n",
    "ipytest.autoconfig(raise_on_error=True)\n",
    "logging.getLogger(\"sagemaker.config\").setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOCAL_MODE = True  # to run the pipelines locally\n",
    "# LOCAL_MODE = False  # to run the pipelines on Sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load environment variables\n",
    "import os\n",
    "\n",
    "bucket = os.environ[\"BUCKET\"]\n",
    "role = os.environ[\"ROLE\"]\n",
    "\n",
    "COMET_API_KEY = os.environ.get(\"COMET_API_KEY\", None)\n",
    "# None means if the value is not available, then None is assigned\n",
    "COMET_PROJECT_NAME = os.environ.get(\"COMET_PROJECT_NAME\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # show the values\n",
    "# bucket, role, COMET_API_KEY, COMET_PROJECT_NAME,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get system OSX architecture\n",
    "architecture = !(uname -m)\n",
    "IS_ARM64_ARCHITECTURE = architecture[0] == \"arm64\"\n",
    "IS_ARM64_ARCHITECTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration dictionary\n",
    "import sagemaker\n",
    "from sagemaker.workflow.pipeline_context import LocalPipelineSession, PipelineSession\n",
    "\n",
    "pipeline_session = PipelineSession(default_bucket=bucket) if not LOCAL_MODE else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check if LOCAL_MODE active\n",
    "LOCAL_MODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'session': <sagemaker.workflow.pipeline_context.LocalPipelineSession at 0x32f0f7f10>,\n",
       " 'instance_type': 'local',\n",
       " 'image': 'sagemaker-tensorflow-toolkit-local',\n",
       " 'framework_version': '2.12',\n",
       " 'py_version': 'py310'}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use LOCAL_MODE w/ docker config if LOCAL_MODE is true\n",
    "if LOCAL_MODE:\n",
    "    config = {\n",
    "        \"session\": LocalPipelineSession(default_bucket=bucket),\n",
    "        \"instance_type\": \"local\",\n",
    "        # use this docker image if arm64 architecture\n",
    "        \"image\": (\n",
    "            (\"sagemaker-tensorflow-toolkit-local\") if IS_ARM64_ARCHITECTURE else None\n",
    "        ),\n",
    "    }\n",
    "else:\n",
    "    config = {\n",
    "        \"session\": pipeline_session,\n",
    "        \"instance_type\": \"ml.m5.xlarge\",\n",
    "        \"image\": None,\n",
    "    }\n",
    "\n",
    "# the below specific settings refer to the sagemaker\n",
    "config[\"framework_version\"] = \"2.12\"\n",
    "config[\"py_version\"] = \"py310\"\n",
    "\n",
    "# check the configuration\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bucket-mlschool-sample\n",
      "s3://bucket-mlschool-sample/penguins\n"
     ]
    }
   ],
   "source": [
    "# define the s3 bucket for this project\n",
    "import boto3\n",
    "\n",
    "S3_LOCATION = f\"s3://{bucket}/penguins\"\n",
    "\n",
    "print(bucket)\n",
    "print(S3_LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'us-east-1'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker_session = sagemaker.session.Session()\n",
    "sagemaker_client = boto3.client(\"sagemaker\")\n",
    "iam_client = boto3.client(\"iam\")\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "region\n",
    "# sagemaker_session.account_id() # shows the account id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Session 3 - Splitting and Transforming the Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - Creating the Preprocessing Script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('code')"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CODE_FOLDER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./code/processing'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f\"./{CODE_FOLDER}/processing\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "(CODE_FOLDER / \"processing\").mkdir(parents=True, exist_ok=True)\n",
    "sys.path.extend([f\"./{CODE_FOLDER}/processing\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting code/processing/script.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {CODE_FOLDER}/processing/script.py\n",
    "# | filename: script.py\n",
    "\n",
    "import os\n",
    "import tarfile\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
    "\n",
    "def _read_data_from_input_csv_files(base_directory):\n",
    "    \"\"\"Read the data from the input CSV files.\n",
    "\n",
    "    This function reads every CSV file available and concatenates them\n",
    "    into a single dataframe.\n",
    "    \"\"\"\n",
    "    input_directory = Path(base_directory)/\"input\"\n",
    "    files = list(input_directory.glob(\"*.csv\"))\n",
    "\n",
    "    if len(files) == 0:\n",
    "        message = f\"There are no CSV files in {input_directory.as_posix()}/\"\n",
    "        raise ValueError(message)\n",
    "\n",
    "    raw_data = [pd.read_csv(file) for file in files]\n",
    "    df = pd.concat(raw_data)\n",
    "\n",
    "    # shuffle data\n",
    "    return df.sample(frac=1, random_state=42)\n",
    "\n",
    "def _split_data(df):\n",
    "    \"\"\"Split the data into train, validation, and test.\"\"\"\n",
    "    df_train, temp = train_test_split(df, test_size=0.3)\n",
    "    df_validation, df_test = train_test_split(temp, test_size=0.5)\n",
    "\n",
    "    return df_train, df_validation, df_test\n",
    "\n",
    "def _save_train_baseline(base_directory, df_train):\n",
    "    \"\"\"Save the untransformed training data to disk.\n",
    "\n",
    "    We will need the training data to compute a baseline to\n",
    "    determine the quality of the data that the model receives\n",
    "    when deployed.\n",
    "    \"\"\"\n",
    "    baseline_path = Path(base_directory)/\"train-baseline\"\n",
    "    baseline_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    df = df_train.copy().dropna()\n",
    "    df = df.drop(\"species\", axis=1)\n",
    "\n",
    "    df.to_csv(baseline_path / \"train-baseline.csv\", header=True, index=False)\n",
    "\n",
    "def _save_test_baseline(base_directory, df_test):\n",
    "    \"\"\"Save the untransformed test data to disk.\n",
    "\n",
    "    We will need the test data to compute a baseline to\n",
    "    determine the quality of the model predictions when deployed.\n",
    "    \"\"\"\n",
    "    baseline_path = Path(base_directory) / \"test-baseline\"\n",
    "    baseline_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    df = df_test.copy().dropna()\n",
    "    df.to_csv(baseline_path / \"test-baseline.csv\", header=False, index=False)\n",
    "\n",
    "def _save_splits(\n",
    "    base_directory,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_validation,\n",
    "    y_validation,\n",
    "    X_test,\n",
    "    y_test\n",
    "):\n",
    "    \"\"\"Save data splits to disk.\n",
    "\n",
    "    This function concatenates the transformed features and the target variable,\n",
    "    saves each one of the split sets to disk.\n",
    "    \"\"\"\n",
    "    # combine X,y\n",
    "    train = np.concatenate((X_train, y_train), axis=1)\n",
    "    validation = np.concatenate((X_validation, y_validation), axis=1)\n",
    "    test = np.concatenate((X_test, y_test), axis=1)\n",
    "\n",
    "    # define paths to save these combined datasets\n",
    "    train_path = Path(base_directory) / \"train\"\n",
    "    validation_path = Path(base_directory) / \"validation\"\n",
    "    test_path = Path(base_directory) / \"test\"\n",
    "\n",
    "    # create directories to save these datasets\n",
    "    train_path.mkdir(parents=True, exist_ok=True)\n",
    "    validation_path.mkdir(parents=True, exist_ok=True)\n",
    "    test_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # save these datasets\n",
    "    pd.DataFrame(train).to_csv(train_path/\"train.csv\", header=False, index=False)\n",
    "    pd.DataFrame(validation).to_csv(validation_path/\"validation.csv\", header=False, index=False)\n",
    "    pd.DataFrame(test).to_csv(test_path/\"test.csv\", header=False,index=False)\n",
    "\n",
    "def _save_model(base_directory, target_transformer, features_transformer):\n",
    "    \"\"\"Save the Scikit-Learn transformation pipeline.\n",
    "\n",
    "    This function creates a model.tar.gz file that\n",
    "    contains the two transformation pipelines\n",
    "    we built to transform the data.\n",
    "    \"\"\"\n",
    "    with tempfile.TemporaryDirectory() as directory:\n",
    "        joblib.dump(target_transformer, Path(directory) / \"target.joblib\")\n",
    "        joblib.dump(features_transformer, Path(directory) / \"features.joblib\")\n",
    "\n",
    "        model_path = Path(base_directory) / \"model\"\n",
    "        model_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        with tarfile.open(f\"{(model_path / 'model.tar.gz').as_posix()}\", \"w:gz\") as tar:\n",
    "            tar.add(Path(directory) / \"target.joblib\", arcname=\"target.joblib\")\n",
    "            tar.add(Path(directory) / \"features.joblib\", arcname=\"features.joblib\")\n",
    "\n",
    "# pipeline to load, split, save data, preprocess, save model\n",
    "def preprocess(base_directory):\n",
    "    \"\"\"Load the supplied data, split it and transform it.\"\"\"\n",
    "    df = _read_data_from_input_csv_files(base_directory)\n",
    "\n",
    "    # define transformers\n",
    "    numeric_transformer = make_pipeline(\n",
    "        SimpleImputer(strategy=\"mean\"),\n",
    "        StandardScaler(),\n",
    "    )\n",
    "\n",
    "    categorical_transformer = make_pipeline(\n",
    "        SimpleImputer(strategy=\"most_frequent\"),\n",
    "        OneHotEncoder(),\n",
    "    )\n",
    "\n",
    "    # use transformers\n",
    "    features_transformer = ColumnTransformer(\n",
    "        transformers=[\n",
    "            (\"numeric\", numeric_transformer, make_column_selector(dtype_exclude=\"object\")),\n",
    "            (\"categorical\", categorical_transformer, [\"island\"]),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    target_transformer = ColumnTransformer(\n",
    "        transformers=[(\"species\", OrdinalEncoder(), [0])]\n",
    "    )\n",
    "\n",
    "    # split data before applying transformers\n",
    "    df_train, df_validation, df_test = _split_data(df)\n",
    "\n",
    "    _save_train_baseline(base_directory, df_train)\n",
    "    _save_test_baseline(base_directory, df_test)\n",
    "\n",
    "    # get target label - apply transformer\n",
    "    y_train = target_transformer.fit_transform(\n",
    "        np.array(df_train[\"species\"].values).reshape(-1,1),\n",
    "    )\n",
    "\n",
    "    y_validation = target_transformer.transform(\n",
    "        np.array(df_validation[\"species\"].values).reshape(-1,1),\n",
    "    )\n",
    "\n",
    "    y_test = target_transformer.transform(\n",
    "        np.array(df_test[\"species\"].values).reshape(-1,1)\n",
    "    )\n",
    "\n",
    "    # get datasets without target variables\n",
    "    df_train = df_train.drop(\"species\", axis=1)\n",
    "    df_validation = df_validation.drop(\"species\", axis=1)\n",
    "    df_test = df_test.drop(\"species\", axis=1)\n",
    "\n",
    "    # get input variables - apply transformers\n",
    "    X_train = features_transformer.fit_transform(df_train) # noqa: N806\n",
    "    X_validation = features_transformer.transform(df_validation) # noqa: N806\n",
    "    X_test = features_transformer.transform(df_test) # noqa: N806\n",
    "\n",
    "    # save the transformed datasets\n",
    "    _save_splits(\n",
    "        base_directory,\n",
    "        X_train,\n",
    "        y_train,\n",
    "        X_validation,\n",
    "        y_validation,\n",
    "        X_test,\n",
    "        y_test\n",
    "    )\n",
    "\n",
    "    # save the transformer models to apply for later use\n",
    "    _save_model(base_directory, target_transformer, features_transformer)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    preprocess(base_directory=\"/opt/ml/processing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the script\n",
    "# start @ 35 cell"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
